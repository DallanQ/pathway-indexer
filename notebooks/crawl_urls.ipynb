{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install requests bs4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../CSVs/stdhndbk.csv')\n",
    "# df2 = pd.read_csv('../CSVs/missionary.csv')\n",
    "# df3 = pd.read_csv('acmsite.csv')\n",
    "# # df2.head(3)\n",
    "# df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def generate_content_hash(content):\n",
    "    '''Generate a SHA-256 hash of the content.'''\n",
    "    return hashlib.sha256(content).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from playwright.async_api import async_playwright\n",
    "async def fetch_content_with_playwright(url, filepath):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        content = await page.content()\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        await browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "async def crawl_csv(input_file, output_file='../data/data_09_12_24/output_data.csv'):\n",
    "    '''Takes CSV file in the format Heading, Subheading, Title, URL and processes each URL.'''\n",
    "    \n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Define a base directory within the user's space\n",
    "    base_dir = '../data/data_09_12_24/crawl/'\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(os.path.join(base_dir, 'html'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_dir, 'pdf'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_dir, 'others'), exist_ok=True)\n",
    "    \n",
    "    output_data = []\n",
    "    \n",
    "    async def process_row(row):\n",
    "        heading = row[0]\n",
    "        sub_heading = row[1]\n",
    "        title = row[2]\n",
    "        url = row[3]\n",
    "        \n",
    "        # Edit the title to become filename\n",
    "        filename = title.replace(' ', '-')\n",
    "        filename = re.sub(r'[^a-zA-Z-]', '', filename)\n",
    "        \n",
    "        # Determine the filepaths\n",
    "        html_filepath = os.path.join(base_dir, 'html', f'{filename}.html')\n",
    "        pdf_filepath = os.path.join(base_dir, 'pdf', f'{filename}.pdf')\n",
    "        \n",
    "        # Skip fetching if the file already exists\n",
    "        if os.path.exists(html_filepath) or os.path.exists(pdf_filepath):\n",
    "            print(f\"File already exists for {title}. Skipping fetch.\")\n",
    "            return\n",
    "        \n",
    "        retry_attempts = 3\n",
    "        \n",
    "        while retry_attempts > 0:\n",
    "            try:\n",
    "                time.sleep(3)  \n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()  # http errors\n",
    "                content_type = response.headers.get('content-type')\n",
    "                \n",
    "                if 'text/html' in content_type:\n",
    "                    content = response.text.encode('utf-8')\n",
    "                    filepath = html_filepath\n",
    "                    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        f.write(response.text)\n",
    "                        \n",
    "                elif 'application/pdf' in content_type:\n",
    "                    content = response.content\n",
    "                    filepath = pdf_filepath\n",
    "                    with open(filepath, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                        \n",
    "                else:\n",
    "                    # Handle other content types by saving with the correct extension\n",
    "                    file_extension = content_type.split('/')[-1].split(';')[0]\n",
    "                    filepath = os.path.join(base_dir, 'others', f'{filename}.{file_extension}')\n",
    "                    content = response.content\n",
    "                    with open(filepath, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                \n",
    "                # Create content hash\n",
    "                content_hash = generate_content_hash(content)\n",
    "                \n",
    "                # Append to the output list\n",
    "                output_data.append([heading, sub_heading, title, url, filepath, content_type.split('/')[1].split(';')[0], content_hash])\n",
    "                break  # Exit retry loop after successful fetch\n",
    "            \n",
    "            except requests.exceptions.HTTPError as http_err:\n",
    "                if response.status_code == 403:\n",
    "                    print(f\"Access forbidden for {url}: {http_err}. Using Playwright to fetch HTML.\")\n",
    "                    html_filepath = os.path.join(base_dir, 'html', f'{filename}.html')\n",
    "                    await fetch_content_with_playwright(url, html_filepath)\n",
    "                    output_data.append([heading, sub_heading, title, url, html_filepath, 'text/html', None])\n",
    "                    break  # Don't retry if it's a 403 error\n",
    "                else:\n",
    "                    print(f\"HTTP error occurred for {url}: {http_err}\")\n",
    "                    retry_attempts -= 1\n",
    "                    if retry_attempts > 0:\n",
    "                        print(f\"Retrying in 10 seconds...\")\n",
    "                        time.sleep(10)\n",
    "                    else:\n",
    "                        output_data.append([heading, sub_heading, title, url, str(http_err), str(response.status_code), None])\n",
    "            \n",
    "            except requests.exceptions.RequestException as err:\n",
    "                print(f\"Error occurred for {url}: {err}\")\n",
    "                retry_attempts -= 1\n",
    "                if retry_attempts > 0:\n",
    "                    print(f\"Retrying in 10 seconds...\")\n",
    "                    time.sleep(10)\n",
    "                else:\n",
    "                    output_data.append([heading, sub_heading, title, url, str(err), 'Error', None])\n",
    "\n",
    "    # Create a list of tasks for asyncio to run\n",
    "    tasks = [process_row(row) for _, row in df.iterrows()]\n",
    "    \n",
    "    # Run the tasks asynchronously\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "    # Create a DataFrame from the output data\n",
    "    output_df = pd.DataFrame(output_data, columns=['Heading', 'Subheading', 'Title', 'URL', 'Filepath', 'Content Type', 'Content Hash'])\n",
    "    \n",
    "    # Append to the existing CSV file or create a new one if it doesn't exist\n",
    "    if os.path.exists(output_file):\n",
    "        output_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        output_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Processing completed. Output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = '../data/data_09_12_24/index/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "  for filename in os.listdir(index_path):\n",
    "    print(filename)\n",
    "    if filename.endswith('.csv'):\n",
    "      print(f'Now handling {filename}!')\n",
    "      await crawl_csv(index_path + filename)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = pd.read_csv('../data/data_09_12_24/output_data.csv')\n",
    "dfout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_09_12_24/output_data.csv')\n",
    "\n",
    "# Filter out rows with '#' in the URL\n",
    "# df_filtered = df[~df['URL'].str.contains('#')]\n",
    "\n",
    "# # Save the filtered DataFrame back to CSV\n",
    "# df_filtered.to_csv('output_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = dfout[(dfout['Content Type'] == '403') | (dfout['Content Type'] == '404')]\n",
    "df_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error.to_csv('../data/data_09_12_24/error_file.csv', mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the len of files in the directory: ../data/data_09_12_24/crawl/html\n",
    "import os\n",
    "path = '../data/data_09_12_24/crawl/html'\n",
    "html_files = os.listdir(path)\n",
    "html_total = str(len(html_files))\n",
    "print('len of files in html: ' + html_total)\n",
    "\n",
    "# read the len of files in the directory: ../data/data_09_12_24/crawl/pdf\n",
    "path = '../data/data_09_12_24/crawl/pdf'\n",
    "pdf_files = os.listdir(path)\n",
    "pdf_total = str(len(pdf_files))\n",
    "print('len of files in pdf: ' + pdf_total)\n",
    "\n",
    "# read the len of files in the directory: ../data/data_09_12_24/crawl/others\n",
    "path = '../data/data_09_12_24/crawl/others'\n",
    "other_files = os.listdir(path)\n",
    "other_totals = str(len(other_files))\n",
    "print('len of files in others: ' + other_totals)\n",
    "\n",
    "total = int(html_total) + int(pdf_total) + int(other_totals)\n",
    "print('Total number of files: ' + str(total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
